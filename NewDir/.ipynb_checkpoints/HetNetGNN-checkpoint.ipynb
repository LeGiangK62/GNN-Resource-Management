{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HZHDjK9CAYl",
    "outputId": "88b1b9d6-6cf9-40ac-a2c7-eae1bedcc726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GNN-Resource-Management'...\n",
      "remote: Enumerating objects: 209, done.\u001b[K\n",
      "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
      "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
      "remote: Total 209 (delta 110), reused 158 (delta 62), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (209/209), 236.41 KiB | 1.27 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n",
      "/content/GNN-Resource-Management/NewDir\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LeGiangK62/GNN-Resource-Management.git\n",
    "%cd GNN-Resource-Management/NewDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euhBKmR3CSf0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "!pip install torch_geometric\n",
    "\n",
    "# Optional dependencies:\n",
    "if torch.cuda.is_available():\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "else:\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PSrar76fCI8E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import Linear, HGTConv\n",
    "\n",
    "from WSN_GNN import generate_channels_wsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cSBfHOVCgbE"
   },
   "source": [
    "# Create HeteroData from the wireless system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lqs7nC0tCOYM"
   },
   "outputs": [],
   "source": [
    "#region Create HeteroData from the wireless system\n",
    "def convert_to_hetero_data(channel_matrices):\n",
    "    graph_list = []\n",
    "    num_sam, num_aps, num_users = channel_matrices.shape\n",
    "    for i in range(num_sam):\n",
    "        x1 = torch.ones(num_users, 1)\n",
    "        x2 = torch.ones(num_users, 1)  # power allocation\n",
    "        x3 = torch.ones(num_users, 1)  # ap selection?\n",
    "        user_feat = torch.cat((x1,x2,x3),1)  # features of user_node\n",
    "        ap_feat = torch.zeros(num_aps, num_aps_features)  # features of user_node\n",
    "        edge_feat_uplink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        edge_feat_downlink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        graph = HeteroData({\n",
    "            'user': {'x': user_feat},\n",
    "            'ap': {'x': ap_feat}\n",
    "        })\n",
    "        # Create edge types and building the graph connectivity:\n",
    "        graph['user', 'uplink', 'ap'].edge_attr = torch.tensor(edge_feat_uplink, dtype=torch.float)\n",
    "        graph['ap', 'downlink', 'user'].edge_attr = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph['user', 'uplink', 'ap'].edge_index = torch.tensor(adj_matrix(num_users, num_aps).transpose(), dtype=torch.int64)\n",
    "        graph['ap', 'downlink', 'user'].edge_index = torch.tensor(adj_matrix(num_aps, num_users).transpose(),\n",
    "                                                                dtype=torch.int64)\n",
    "\n",
    "        # graph['ap', 'downlink', 'user'].edge_attr  = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def adj_matrix(num_from, num_dest):\n",
    "    adj = []\n",
    "    for i in range(num_from):\n",
    "        for j in range(num_dest):\n",
    "            adj.append([i, j])\n",
    "    return np.array(adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMJBmS42Ckc-"
   },
   "source": [
    "# Build Heterogeneous GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "3Oigpa4mCVBk"
   },
   "outputs": [],
   "source": [
    "#region Build Heterogeneous GNN\n",
    "class HetNetGNN(torch.nn.Module):\n",
    "    def __init__(self, data, hidden_channels, out_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           num_heads, group='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        original = x_dict['user'].clone\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        original = x_dict['user'] # not original\n",
    "        power = self.lin(x_dict['user'])\n",
    "        ap_selection = self.lin1(x_dict['user'])\n",
    "        ap_selection = torch.abs(ap_selection).int()\n",
    "#         print(original, power, ap_selection)\n",
    "        print(x for node_type, x in x_dict.items())\n",
    "        \n",
    "        out = torch.cat((original[:,1].unsqueeze(-1), power[:,1].unsqueeze(-1), ap_selection[:,1].unsqueeze(-1)), 1)\n",
    "        return out\n",
    "\n",
    "class EdgeConv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def foward(self, graph, inputs):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class RoundActivation(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.round(torch.abs(x))\n",
    "\n",
    "#endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xARv-2yLCnwN"
   },
   "source": [
    "# Training and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "3zupX9hyCXoc"
   },
   "outputs": [],
   "source": [
    "#region Training and Testing functions\n",
    "def loss_function(output, batch, is_train=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_user = batch['user']['x'].shape[0]\n",
    "    num_ap = batch['ap']['x'].shape[0]\n",
    "    ##\n",
    "    channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "    ##\n",
    "#     power_max = batch['user']['x'][:, 0]\n",
    "#     power = batch['user']['x'][:, 1]\n",
    "#     ap_selection = batch['user']['x'][:, 2]\n",
    "    power_max = output[:, 0]\n",
    "    power = output[:, 1]\n",
    "    ap_selection = output[:, 2]\n",
    "    ##\n",
    "    ap_selection = ap_selection.int()\n",
    "    index = torch.arange(num_user)\n",
    "\n",
    "    G = torch.reshape(channel_matrix, (-1, num_ap, num_user))\n",
    "    # P = torch.reshape(power, (-1, num_ap, num_user)) #* p_max\n",
    "    P = torch.zeros_like(G, requires_grad=True).clone()\n",
    "    P[0, ap_selection[index], index] = power_max * power\n",
    "    ##\n",
    "    # new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    desired_signal = torch.sum(torch.mul(P, G), dim=1).unsqueeze(-1)\n",
    "    G_UE = torch.sum(G, dim=2).unsqueeze(-1)\n",
    "    all_signal = torch.matmul(P.permute((0,2,1)), G_UE)\n",
    "    interference = all_signal - desired_signal #+ new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P.permute((0,2,1)), 1))\n",
    "\n",
    "    if is_train:\n",
    "        return torch.neg(sum_rate / mean_power)\n",
    "    else:\n",
    "        return sum_rate / mean_power\n",
    "\n",
    "\n",
    "\n",
    "def train(data_loader):\n",
    "    model.train()\n",
    "    device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device_type)\n",
    "        # batch_size = batch['user'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        tmp_loss = loss_function(out, batch, True)\n",
    "        tmp_loss.backward()\n",
    "        optimizer.step()\n",
    "        #total_examples += batch_size\n",
    "        total_loss += float(tmp_loss) #* batch_size\n",
    "\n",
    "    return total_loss #/ total_examples\n",
    "\n",
    "\n",
    "def test(data_loader):\n",
    "    model.eval()\n",
    "    device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device_type)\n",
    "        # batch_size = batch['user'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        tmp_loss = loss_function(out, batch, False)\n",
    "        #total_examples += batch_size\n",
    "        total_loss += float(tmp_loss) #* batch_size\n",
    "\n",
    "    return total_loss #/ total_examples\n",
    "#endregion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYnfbYmsCa59"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7acs4Y0uCdoD"
   },
   "outputs": [],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_users_features = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "num_train = 2  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hCH-zKEADSQ8"
   },
   "outputs": [],
   "source": [
    "# Maybe need normalization here\n",
    "train_data = convert_to_hetero_data(X_train)\n",
    "test_data = convert_to_hetero_data(X_test)\n",
    "\n",
    "batchSize = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batchSize, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data, batchSize, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "0_RqWpsQDVmV"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = train_data[0]\n",
    "data = data.to(device)\n",
    "\n",
    "model = HetNetGNN(data, hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# # print(data.edge_index_dict)\n",
    "# with torch.no_grad():\n",
    "#     output = model(data.x_dict, data.edge_index_dict)\n",
    "# print(output)\n",
    "# print(data)\n",
    "\n",
    "# data = test_data[0]\n",
    "# data = data.to(device)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     output = model(data.x_dict, data.edge_index_dict)\n",
    "#     print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJYiXcE9Cs2M"
   },
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "_AdzgAL4CqOM",
    "outputId": "e588b0d7-1a7e-4916-c594-ed5fd14d4add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7970>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7970>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7C10>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "Epoch: 001, Train Loss: -351.9912, Test Reward: 1687.1417\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB79E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB79E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB74A0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "Epoch: 002, Train Loss: -23293.4414, Test Reward: 8442.3735\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7970>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7970>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158EFB7C10>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "<generator object HetNetGNN.forward.<locals>.<genexpr> at 0x000002158C6EA9E0>\n",
      "Epoch: 003, Train Loss: -5692.9960, Test Reward: 886.5392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[165], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_loader)\u001b[0m\n\u001b[0;32m     42\u001b[0m device_type \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m total_examples \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     46\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device_type)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Reward: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "QTIh-q4IDXsU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]), 'ap': tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = train_data[0]\n",
    "data = data.to(device)\n",
    "\n",
    "model = HetNetGNN(data, hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "total_examples = total_loss = 0\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    batch = batch.to(device_type)\n",
    "    # batch_size = batch['user'].batch_size\n",
    "    break\n",
    "print(batch.x_dict)\n",
    "# out = model(batch.x_dict, batch.edge_index_dict)\n",
    "# print(out.shape)\n",
    "# tmp_loss = loss_function(out, data, True)\n",
    "# print(tmp_loss)\n",
    "# tmp_loss.backward()\n",
    "# # Print computation graph for debugging\n",
    "# print(\"Computation Graph:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.grad is not None:\n",
    "#         print(name, param.grad.abs().sum())\n",
    "\n",
    "# # optimizer.step()\n",
    "# # #total_examples += batch_size\n",
    "# # total_loss += float(tmp_loss) #* batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_user = batch['user']['x'].shape[0]\n",
    "num_ap = batch['ap']['x'].shape[0]\n",
    "##\n",
    "channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "power_max = batch['user']['x'][:, 0]\n",
    "power = batch['user']['x'][:, 1]\n",
    "ap_selection = batch['user']['x'][:, 2]\n",
    "ap_selection = ap_selection.int()\n",
    "index = torch.arange(num_user)\n",
    "\n",
    "G = torch.reshape(channel_matrix, (-1, num_ap, num_user))\n",
    "# P = torch.reshape(power, (-1, num_ap, num_user)) #* p_max\n",
    "P = torch.zeros_like(G, requires_grad=True).clone()\n",
    "P[0, ap_selection[index], index] = power_max * power\n",
    "##\n",
    "# new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "desired_signal = torch.sum(torch.mul(P, G), dim=1).unsqueeze(-1)\n",
    "G_UE = torch.sum(G, dim=2).unsqueeze(-1)\n",
    "all_signal = torch.matmul(P.permute((0,2,1)), G_UE)\n",
    "interference = all_signal - desired_signal #+ new_noise\n",
    "rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "mean_power = torch.mean(torch.sum(P.permute((0,2,1)), 1))\n",
    "\n",
    "if is_train:\n",
    "    return torch.neg(sum_rate / mean_power)\n",
    "else:\n",
    "    return sum_rate / mean_power"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
