{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:52.595448600Z",
     "start_time": "2023-07-13T02:02:52.577445100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Giang\\Code\\GNN-Resource-Management\\NewDir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current folder\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "# Print the current folder\n",
    "print(current_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:53.287883600Z",
     "start_time": "2023-07-13T02:02:53.279882200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the desired directory path\n",
    "new_directory = 'D:/Giang/Code/GNN-Resource-Management/NewDir'\n",
    "\n",
    "# Change the current directory\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:53.828475500Z",
     "start_time": "2023-07-13T02:02:53.802469800Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "# Load .mat file\n",
    "data = scipy.io.loadmat('Allocation_8Ids_3APs.mat')\n",
    "\n",
    "# Access the variables\n",
    "Mu = data['Mu']\n",
    "Tau = data['Tau']\n",
    "Power = data['Power']\n",
    "ChannelAll = data['ChannelAll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:00:14.900182Z",
     "start_time": "2023-07-13T02:00:14.891180500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.19693747e+00, 1.48189778e+01, 5.76762579e+00, 6.77059628e-06,\n",
       "        6.19523472e+00, 1.08581442e-08, 1.32132602e+01, 4.76384006e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.44011078e-07, 2.64449106e-05, 2.98598317e-06],\n",
       "       [2.15862365e-07, 6.78765907e-05, 1.19251445e-06],\n",
       "       [8.11006265e-07, 5.95050178e-06, 7.64526091e-06],\n",
       "       [2.56285471e-07, 8.60366171e-07, 1.31314600e-06],\n",
       "       [1.26782153e-06, 4.70406099e-07, 6.19311804e-07],\n",
       "       [1.82246142e-06, 2.20294781e-07, 2.47949891e-06],\n",
       "       [4.20545451e-06, 2.24559726e-07, 1.26098801e-06],\n",
       "       [4.05121889e-06, 9.69237742e-07, 7.51929511e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChannelAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test WSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN\n",
    "\n",
    "from reImplement import GCNet\n",
    "from setup_arguments import setup_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_channels_wsn(num_ap, num_user, num_samples, var_noise=1.0, radius=1):\n",
    "    # print(\"Generating Data for training and testing\")\n",
    "\n",
    "    # if num_ap != 1:\n",
    "    #     raise Exception(\"Can not generate data for training and testing with more than 1 base station\")\n",
    "    # generate position\n",
    "    dist_mat = []\n",
    "    position = []\n",
    "    index_user = np.tile(np.arange(num_user), (num_ap, 1))\n",
    "    index_ap = np.tile(np.arange(num_ap).reshape(-1, 1), (1, num_user))\n",
    "\n",
    "    index = np.array([index_user, index_ap])\n",
    "\n",
    "    # Calculate channel\n",
    "    CH = 1 / np.sqrt(2) * (np.random.randn(num_samples, 1, num_user)\n",
    "                           + 1j * np.random.randn(num_samples, 1, num_user))\n",
    "\n",
    "    if radius == 0:\n",
    "        Hs = abs(CH)\n",
    "    else:\n",
    "        for each_sample in range(num_samples):\n",
    "            pos = []\n",
    "            pos_BS = []\n",
    "\n",
    "            for i in range(num_ap):\n",
    "                r = radius * (np.random.rand())\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_BS.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "            pos_user = []\n",
    "\n",
    "            for i in range(num_user):\n",
    "                r = 0.5 * radius + 0.5 * radius * np.random.rand()\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_user.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            pos_BS = np.array(pos_BS)\n",
    "            dist_matrix = distance_matrix(pos_BS, pos_user)\n",
    "            # dist_matrixp = distance_matrix(pos[1:], pos[1:])\n",
    "            dist_mat.append(dist_matrix)\n",
    "            position.append(pos)\n",
    "\n",
    "        dist_mat = np.array(dist_mat)\n",
    "        position = np.array(position)\n",
    "\n",
    "        # Calculate Free space pathloss\n",
    "        # f = 2e9\n",
    "        # c = 3e8\n",
    "        # FSPL_old = 1 / ((4 * np.pi * f * dist_mat / c) ** 2)\n",
    "        FSPL = - (120.9 + 37.6 * np.log10(dist_mat/1000))\n",
    "        FSPL = 10 ** (FSPL / 10)\n",
    "\n",
    "        # print(f'FSPL_old:{FSPL_old.sum()}')\n",
    "        # print(f'FSPL_new:{FSPL.sum()}')\n",
    "        Hs = abs(CH * FSPL)\n",
    "\n",
    "    adj = adj_matrix(num_user * num_ap)\n",
    "\n",
    "    Hs, noise = normalize_matrix(Hs, var_noise)\n",
    "\n",
    "    return Hs, noise, position, adj, index\n",
    "\n",
    "def adj_matrix(num_nodes):\n",
    "    adj = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if not (i == j):\n",
    "                adj.append([i, j])\n",
    "    return np.array(adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_build(channel_matrix, index_matrix):\n",
    "    num_user, num_ap = channel_matrix.shape\n",
    "    adjacency_matrix = adj_matrix(num_user * num_ap)\n",
    "\n",
    "    index_user = np.reshape(index_matrix[0], (-1, 1))\n",
    "    index_ap = np.reshape(index_matrix[1], (-1, 1))\n",
    "\n",
    "    x1 = np.reshape(channel_matrix, (-1, 1))\n",
    "#     x2 = np.ones((num_user * num_ap, 1)) # power max here, for each?\n",
    "    x2 = np.zeros((num_user * num_ap, 1)) # ap selection\n",
    "    x3 = np.zeros((num_user * num_ap, 1))\n",
    "    x = np.concatenate((x1, x2, x3),axis=1)\n",
    "\n",
    "    edge_index = adjacency_matrix\n",
    "    edge_attr = []\n",
    "\n",
    "    for each_interference in adjacency_matrix:\n",
    "        tx = each_interference[0]\n",
    "        rx = each_interference[1]\n",
    "\n",
    "        tmp = [channel_matrix[index_ap[rx][0]][index_user[tx][0]]]\n",
    "#         tmp = [\n",
    "#             [channel_matrix[index_ap[rx][0]][index_user[tx][0]]],\n",
    "#             [channel_matrix[index_ap[tx][0]][index_user[rx][0]]]\n",
    "#         ]\n",
    "        edge_attr.append(tmp)\n",
    "\n",
    "    # y = np.expand_dims(channel_matrix, axis=0)\n",
    "    # pos = np.expand_dims(weights_matrix, axis=0)\n",
    "\n",
    "    data = Data(x=torch.tensor(x, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
    "                edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "                # y=torch.tensor(y, dtype=torch.float),\n",
    "                # pos=torch.tensor(pos, dtype=torch.float)\n",
    "                )\n",
    "    return data\n",
    "\n",
    "def build_all_data(channel_matrices, index_mtx):\n",
    "    num_sample = channel_matrices.shape[0]\n",
    "    data_list = []\n",
    "    for i in range(num_sample):\n",
    "        data = graph_build(channel_matrices[i], index_mtx)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def data_rate_calc(data, out, num_ap, num_user, noise_matrix, p_max, train = True, isLog=False):\n",
    "    G = torch.reshape(out[:, 0], (-1, num_ap, num_user))  #/ noise\n",
    "    ap_select = torch.reshape(out[:, 2], (-1, num_ap, num_user))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # how to get channel from data and output\n",
    "    P = torch.reshape(out[:, 2], (-1, num_ap, num_user)) * p_max\n",
    "    P = torch.mul(P,ap_select)\n",
    "    desired_signal = torch.sum(torch.mul(P,G), dim=2).unsqueeze(-1)\n",
    "    P_UE = torch.sum(P, dim=1).unsqueeze(-1)\n",
    "    all_received_signal = torch.matmul(G, P_UE)\n",
    "    new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    interference = all_received_signal - desired_signal + new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P_UE, 1))\n",
    "\n",
    "    if(isLog):\n",
    "      print(f'Channel Coefficient: {G}')\n",
    "      print(f'Power: {P}')\n",
    "      print(f'desired_signal: {desired_signal}')\n",
    "      print(f'P_UE: {P_UE}')\n",
    "      print(f'all_received_signal: {all_received_signal}')\n",
    "      print(f'interference: {interference}')\n",
    "\n",
    "    if train:\n",
    "        return torch.neg(sum_rate/mean_power)\n",
    "    else:\n",
    "        return sum_rate/mean_power\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_train = 2  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data in to graph structured for model\n",
    "train_data_list = build_all_data(X_train, index_train)\n",
    "test_data_list = build_all_data(X_test, index_test)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=10, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data_list, batch_size=10, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "testing_loss = []\n",
    "# Training and Testing model\n",
    "for epoch in range(1, 100):\n",
    "    total_loss = 0\n",
    "    for each_data in train_loader:\n",
    "        data = each_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = data_rate_calc(data, out, K, N, noise_train, power_threshold, train=True)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = total_loss / num_train\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for each_data in test_loader:\n",
    "        data = each_data.to(device)\n",
    "        out = model(data)\n",
    "        loss = data_rate_calc(data, out, K + 1, N + 10, noise_test, power_threshold,  train=False)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    test_loss = total_loss / num_test\n",
    "\n",
    "    training_loss.append(train_loss)\n",
    "    testing_loss.append(test_loss)\n",
    "    if (epoch % 8 == 1):\n",
    "        print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "            epoch, train_loss, test_loss))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.num_features: 1433\n",
      "dataset.num_classes : 7\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Planetoid' object has no attribute 'num_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_classes : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_nodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_classes : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:175\u001b[0m, in \u001b[0;36mInMemoryDataset.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    172\u001b[0m         data_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()]\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_list)[key]\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Planetoid' object has no attribute 'num_nodes'"
     ]
    }
   ],
   "source": [
    "print(f'dataset.num_features: {dataset.num_features}')\n",
    "print(f'dataset.num_classes : {dataset.num_classes }')\n",
    "print(f'dataset.num_features: {dataset.num_nodes}')\n",
    "print(f'dataset.num_classes : {dataset.num_classes }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogenous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "num_users = 5\n",
    "num_users_features = 3\n",
    "\n",
    "num_aps = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "# Create two node types \"paper\" and \"author\" holding a feature matrix:\n",
    "data['user'].x = torch.randn(num_users, num_users_features) # features of user_node\n",
    "data['ap'].x = torch.randn(num_aps, num_aps_features) # features of user_node\n",
    "\n",
    "# Create edge types and building the\n",
    "# graph connectivity:\n",
    "data['user', 'up', 'ap'].edge_index = ...  # [2, num_edges]\n",
    "data['ap', 'down', 'user'].edge_index = ...  # [2, num_edges]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ap'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user', 'up', 'ap'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ap', 'user'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1muser\u001b[0m={ x=[5, 3] },\n",
       "  \u001b[1map\u001b[0m={ x=[3, 3] },\n",
       "  \u001b[1mpaper\u001b[0m={},\n",
       "  \u001b[1m(user, up, ap)\u001b[0m={ edge_index=Ellipsis },\n",
       "  \u001b[1m(ap, down, user)\u001b[0m={ edge_index=Ellipsis }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dict = {\n",
    "    'ap': torch.tensor([0, 1]),\n",
    "    'user': torch.tensor([0, 2]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': tensor([[ 1.6823,  0.8296,  1.7004],\n",
      "        [-0.5770, -1.0360, -1.2604],\n",
      "        [-0.0759,  1.3773, -0.9175],\n",
      "        [ 0.2248,  0.8405,  1.3123],\n",
      "        [ 0.9722, -0.1851,  0.0476]]), 'ap': tensor([[-0.9269,  1.0285,  0.5206],\n",
      "        [-1.2664, -0.6786, -0.1242],\n",
      "        [ 0.8528, -2.1334, -0.3032]])}\n"
     ]
    }
   ],
   "source": [
    "print(data.collect('x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy  # for testing\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from WSN_GNN import generate_channels_wsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 5\n",
    "num_users_features = 3\n",
    "\n",
    "num_aps = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "\n",
    "def convert_to_hetero_data(channel_matrices):\n",
    "    graph_list = []\n",
    "    num_sam, num_aps, num_users = channel_matrices.shape\n",
    "    for i in range(num_sam):\n",
    "        user_feat = torch.zeros(num_users, num_users_features)  # features of user_node\n",
    "        ap_feat = torch.zeros(num_aps, num_aps_features)  # features of user_node\n",
    "        edge_feat_uplink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        edge_feat_downlink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        graph = HeteroData({\n",
    "            'user': {'x': user_feat},\n",
    "            'ap': {'x': ap_feat}\n",
    "        })\n",
    "        # Create edge types and building the graph connectivity:\n",
    "        graph['user', 'uplink', 'ap'].edge_attr  = torch.tensor(edge_feat_uplink, dtype=torch.float)\n",
    "        graph['ap', 'downlink', 'user'].edge_attr  = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'edge_index': tensor([[6.0807e-03],\n",
      "        [3.4064e-01],\n",
      "        [1.0000e+00],\n",
      "        [3.2971e-02],\n",
      "        [3.6708e-03],\n",
      "        [5.2365e-03],\n",
      "        [4.7612e-03],\n",
      "        [8.1164e-03],\n",
      "        [2.1373e-02],\n",
      "        [2.8898e-03],\n",
      "        [3.5284e-03],\n",
      "        [6.0557e-02],\n",
      "        [6.6312e-02],\n",
      "        [8.8646e-03],\n",
      "        [9.2885e-04]])}, {'edge_index': tensor([[6.0807e-03],\n",
      "        [3.4064e-01],\n",
      "        [1.0000e+00],\n",
      "        [3.2971e-02],\n",
      "        [3.6708e-03],\n",
      "        [5.2365e-03],\n",
      "        [4.7612e-03],\n",
      "        [8.1164e-03],\n",
      "        [2.1373e-02],\n",
      "        [2.8898e-03],\n",
      "        [3.5284e-03],\n",
      "        [6.0557e-02],\n",
      "        [6.6312e-02],\n",
      "        [8.8646e-03],\n",
      "        [9.2885e-04]])}]\n"
     ]
    }
   ],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_train = 4  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)\n",
    "\n",
    "train_data = convert_to_hetero_data(X_train)\n",
    "print(train_data[0].edge_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
